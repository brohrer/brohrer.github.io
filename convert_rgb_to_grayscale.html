<!DOCTYPE html>
<html>

  <script type="text/javascript">var blog_title = "How to Convert an RGB Image to Grayscale";</script>
  <script type="text/javascript">var publication_date = "November 14, 2019";</script>
  <head>
    <link rel="icon" href="images/ml_logo.png">
    <meta HTTP-EQUIV="REFRESH" content="0; url=https://e2eml.school/convert_rgb_to_grayscale.html">
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <base target="_blank">
    <script type="text/javascript" src="javascripts/blog_head.js"></script>
  </head>
  <body>
    <script type="text/javascript" src="javascripts/blog_header.js"></script>
    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <p>
          Occasionally the need arises to convert a color image to grayscale.
          This need came up when loading images taken on the surface
          of Mars as part of
          <a href="https://end-to-end-machine-learning.teachable.com/p/advanced-neural-network-methods">
          End-to-End Machine Learning Course 313, Advanced Neural Network
          Methods</a>. We were working with a mixture of color and
          grayscale images and needed to transform them into a uniform
          format - all grayscale.
          We'll be working in Python using the Pillow,
          Numpy, and Matplotlib
          packages.
        </p>
        <p>
          By the way, all the interesting information in this post all comes from
          <a href="https://en.wikipedia.org/wiki/Grayscale">
            the Wikipedia entry on Grayscale</a>. (If you find it helpful,
          <a href="https://donate.wikimedia.org/w/index.php?title=Special:LandingPage&country=US&uselang=en&utm_medium=sidebar&utm_source=donate&utm_campaign=C13_en.wikipedia.org">
            maybe send them a dollar</a>.)
        </p>
        <h3>Read in a color image</h3>
        <p>
          The <a href="https://github.com/brohrer/cottonwood_martian_images/blob/master/image_loader.py">
            code we're working from</a> loads jpeg images for an autoencoder
            to use as inputs. This is accomplished with using Pillow and Numpy:
        </p>
        <p>
<pre><code>from PIL import Image
import numpy as np
color_img = np.asarray(Image.open(img_filename)) / 255</code></pre>
        </p>
        <p>
          This reads the image in and converts it into a Numpy array.
          For a detailed description of what this does and why, check
          out the prequel post to this one:
          <a href="https://e2eml.school/images_to_numbers.html">
            How to Convert a Picture into Numbers</a>.
          For grayscale images, the result is a two-dimensional array with the
          number of rows and columns equal to the number of pixel rows
          and columns in the image. Low numeric values indicate
          darker shades and higher values lighter shades. The range
          of pixel values is often 0 to 255. We divide by 255 to
          get a range of 0 to 1.
        </p>
        <p>
          Color images are represented as three-dimensional Numpy arrays -
          a collection of three two-dimensional arrays, one each for red,
          green, and blue channels. Each one, like grayscale arrays,
          has one value per pixel and their ranges are identical.
        </p>
        <p style="text-align:center;">
          <img title="Three dimensional image array structure"
            src="images/image_processing/three_d_array.png"
            alt="Three dimensional image array structure"
            style="height: 320px;">
        </p>

        <p style="text-align:center;">
        <img title="Color image with red, green, and blue channels"
          src="images/image_processing/reign_pic_breakdown.png"
          alt="Color image with red, green, and blue channels"
          style="height: 380px;">
          Image credit: Diane Rohrer
        </p>

        <h3>Easy peasy: Average the channels</h3>
        <p>
          An intuitive way to convert a color image 3D array to a grayscale
          2D array is, for each pixel, take the average of the red, green,
          and blue pixel values to get the grayscale value. This combines
          the lightness or <em>luminance</em> contributed by each color band into
          a reasonable gray approximation.
        </p>
        <p>
          <code>img = numpy.mean(color_img, axis=2)</code>
        </p>
        <p>
          The <code>axis=2</code> argument tells <code>numpy.mean()</code> to
          average values across all three color channels.
          (<code>axis=0</code> would average across pixel rows and
          <code>axis=1</code> would average across pixel columns.)
        </p>

        <p style="text-align:center;">
        <img title="Gray from averaging red, green, and blue channels"
          src="images/image_processing/gray_mean.png"
          alt="Gray from averaging red, green, and blue channels"
          style="height: 320px;">
        </p>

        <h3>Well, actually...channel-dependent luminance perception</h3>
        <p>
          To our eyes green looks abount ten times
          brighter than blue. Through many
          repetitions of carefully designed experiments, psychologists
          have figured out how different we perceive the luminance or
          red, green, and blue to be. They have provided us a different
          set of weights for our channel averaging to get total luminance.
        </p>
        <p style="text-align:center;">
        <a href="https://en.wikipedia.org/wiki/Grayscale#Colorimetric_(perceptual_luminance-preserving)_conversion_to_grayscale">
        <img title="Equation for combinging red, green, and blue channels"
          src="images/image_processing/rgb2gray_eq_linear.png"
          alt="Equation for combining red, green, and blue channels"
          style="height: 50px;">
        </a>
        </p>
        <p>
          The results are noticeably different and, to my eye, more accurate.
        </p>

        <p style="text-align:center;">
        <img title="Gray from weighted averaging red, green, and blue channels"
          src="images/image_processing/gray_lin.png"
          alt="Gray from weighted averaging red, green, and blue channels"
          style="height: 500px;">
        </p>

        <h3>Well, actually...gamma compression</h3>
        <p>
          We are able to see small differences when luminance is low,
          but at high luminance levels, we are much less sensitive to them.
          In order to avoid wasting effort representing imperceptible
          differences at high luminance, the color scale is warped,
          so that it concentrates more values in the lower end of the range,
          and spreads them out more widely in the higher end. This is
          called gamma compression.
        </p>
        <p>
          To undo the effects of gamma compression before calculating the
          grayscale luminance, it's necessary to apply the inverse
          operation, gamma expansion:
        </p>

        <p style="text-align:center;">
        <a href="https://en.wikipedia.org/wiki/Grayscale#Colorimetric_(perceptual_luminance-preserving)_conversion_to_grayscale">
        <img title="Nonlinear transformation of gamma"
          src="images/image_processing/gamma_compression_eq.png"
          alt="Nonlinear transformation of gamma"
          style="height: 130px;">
        </a>
        </p>
        <p style="text-align:center;">
        <img title="Gamma compression function"
          src="images/image_processing/gamma_fcn.png"
          alt="Gamma compression function"
          style="height: 350px;">
        </p>
        <p>
          The benefit of gamma compression is that it gets rid of
          banding in smoothly varying dark colors, like a photo of the sky
          at twilight. The downside is that if we want to do anything
          like adding, subtracting, or averaging bands, we first have
          to undo the compression and get the luminance
          back into a linear representation.
        </p>

        <p style="text-align:center;">
        <img title="Gray accounting for gamma compression"
          src="images/image_processing/gray_nl.png"
          alt="Gray accounting for gamma compression"
          style="height: 500px;">
        </p>

        <p>
          There is lightening throughout the image after accounting
          for gamma compression. It brings the luminance up to be a closer
          match to that of the original image. Finally, we have a high
          quality grayscale representation.
        </p>

        <h3>Well, actually...a linear approximation</h3>
        <p>
          The gamma decompresssion and re-compression rack up quite a large
          computation cost, compared to the weighted averages we were
          working with before. Sometimes speed is more desirable than 
          accurate-as-possible luminance calculations. For situations
          like these, there is a linear approximation:
        </p>

        <p style="text-align:center;">
        <img title="Equation for combinging red, green, and blue channels"
          src="images/image_processing/rgb2gray_eq_approx.png"
          alt="Equation for combining red, green, and blue channels"
          style="height: 40px;">
        </p>

        <p>
          This lets you get a result that's a little closer to the
          gamma-compression-corrected version, but without the extra
          computation time.
        </p>
          
        <p style="text-align:center;">
        <img title="Linear approximation of gray accounting for gamma compression"
          src="images/image_processing/gray_approx.png"
          alt="Linear approximation of gray accounting for gamma compression"
          style="height: 500px;">
        </p>

        <p>
          As you can see, the results are not bad at all. They tend
          to be a little darker, especially through the
          red mid-range values, but arguably just as good in most
          practical respects.
        </p>
        <p>
          This method of calculating luminance is codified in the 
          standard 
          <a href="https://www.itu.int/dms_pubrec/itu-r/rec/bt/R-REC-BT.601-7-201103-I!!PDF-E.pdf">
            ITU-R BT.601 Studio encoding parameters of digital television
            for standard 4:3 and wide screen 16:9 aspect ratios. </a>
          which incidentally was awaded an Emmy in 1983.
        </p>

        <h3>Which one should I use?</h3>
        <p>
          If close is good enough or if you really care about speed,
          use the linear approximation of gamma correction.
          This is the approach used by
          <a href="https://www.mathworks.com/help/matlab/ref/rgb2gray.html">
            MATLAB</a>,
          <a href="https://pillow.readthedocs.io/en/3.1.x/reference/Image.html">
            Pillow</a>, and
          <a href="https://github.com/opencv/opencv/blob/8c0b0714e76efef4a8ca2a7c410c60e55c5e9829/modules/imgproc/src/color.simd_helpers.hpp">
            OpenCV</a>.
          It is included in my
          <a href="
          https://github.com/brohrer/lodgepole/blob/master/lodgepole/image_tools.py
          ">
            Lodgepole image and video processing toolbox</a>:
        </p>
        <p>
<pre><code>import lodgepole.image_tools as lit
gray_img = lit.rgb2gray_approx(color_img)
</code></pre>
        </p>
        <p>
          But if you simply must have the best results, splurge on
          the whole gamma decompression - perceptual luminance corrected - 
          gamma re-compression pipeline:
        </p>
        <p>
<pre><code>import lodgepole.image_tools as lit
gray_img = lit.rgb2gray(color_img)
</code></pre>
        </p>
        <p>
          If after reading this far
          you insist on straight up averaging the three channels together,
          I will judge you.
        </p>
        <p>
          Now go make beautiful grayscale images!
        </p>

        <script type="text/javascript" src="javascripts/blog_signature.js"></script>
      </section>
    </div>
    <script type="text/javascript" src="javascripts/blog_footer.js"></script>
    <script type="text/javascript">
      var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
      document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
      try {
        var pageTracker = _gat._getTracker("UA-10180621-3");
      pageTracker._trackPageview();
      } catch(err) {}
    </script>
  </body>
</html>
